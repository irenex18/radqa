{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imx2/miniconda3/envs/myenv/lib/python3.8/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
      "  _C._set_default_tensor_type(t)\n",
      "[nltk_data] Downloading package punkt to /home/imx2/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import re\n",
    "from datasets import load_dataset, Dataset, load_metric\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    "    LogitsProcessor,\n",
    "    LogitsProcessorList,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "import nltk\n",
    "import warnings\n",
    "import spacy\n",
    "import math\n",
    "import xformers\n",
    "import tensor_parallel as tp\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up model and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# we save the model under this name\n",
    "new_model = \"llama-2-7b-radnlpv2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.25\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 3\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_scheduler_type = \"cosine\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 0\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "# Evaluate model on validation set every X update steps\n",
    "evaluation_strategy='steps'\n",
    "eval_steps=500\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = 200\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# batch size\n",
    "dataset_batch_size = 32\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "# device_map = {'':torch.cuda.current_device()}\n",
    "# CUDA_VISIBLE_DEVICES=0,1,2,3, try multiple devices\n",
    "device_map = 'auto' #{'':torch.cuda.current_device()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer and model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e7b2e59f744669806fe3c6dc0e9653",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "# model = tp.tensor_parallel(\n",
    "#     AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     quantization_config=bnb_config,\n",
    "# #     device_map=device_map\n",
    "#     )\n",
    "# )\n",
    "# model = tp.tensor_parallel(model, [\"cuda:0\", \"cuda:1\"])  # <- each GPU has half the weights\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    evaluation_strategy=evaluation_strategy,\n",
    "    eval_steps=eval_steps,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERTSCORE_MODEL_TYPE = \"microsoft/deberta-xlarge-mnli\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_sci_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune Set Up\n",
    "Function where inputs are different hyperparameters that can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EosTokenRewardLogitProcess(LogitsProcessor):\n",
    "    # class to get the model to generate EOS token more often as sentence nears max_length\n",
    "    def __init__(self, eos_token_id: int, max_length: int):\n",
    "        if not isinstance(eos_token_id, int) or eos_token_id < 0:\n",
    "            raise ValueError(f\"`eos_token_id` has to be a positive integer, but is {eos_token_id}\")\n",
    "\n",
    "        if not isinstance(max_length, int) or max_length < 1:\n",
    "            raise ValueError(f\"`max_length` has to be a integer bigger than 1, but is {max_length}\")\n",
    "\n",
    "        self.eos_token_id = eos_token_id\n",
    "        self.max_length=max_length\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        cur_len = input_ids.shape[-1]\n",
    "        # start to increese the reward of the  eos_tokekn from 70% max length  progressively on length\n",
    "        for cur_len in (max(0,int(self.max_length*0.7)), self.max_length ):\n",
    "            ratio = cur_len/self.max_length\n",
    "            num_tokens = scores.shape[1] # size of vocab\n",
    "            scores[:, [i for i in range(num_tokens) if i != self.eos_token_id]] =\\\n",
    "            scores[:, [i for i in range(num_tokens) if i != self.eos_token_id]]*ratio*10*torch.exp(-torch.sign(scores[:, [i for i in range(num_tokens) if i != self.eos_token_id]]))\n",
    "            scores[:, self.eos_token_id] = 1.1e2 * ratio\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(model, train_dataset, eval_dataset, peft_config, max_seq_length, tokenizer, training_arguments, packing, formatting_func, new_model, compute_metrics, preprocess_logits_for_metrics):\n",
    "    # Set supervised fine-tuning parameters\n",
    "    # add validation set to model\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        peft_config=peft_config,\n",
    "        max_seq_length=max_seq_length,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_arguments,\n",
    "        packing=packing,\n",
    "        formatting_func=formatting_func,\n",
    "        compute_metrics=compute_metrics,\n",
    "        preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    trainer.train()\n",
    "\n",
    "    # Save trained model\n",
    "    trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model\n",
    "Functions to evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rouge_scores(predictions, references):\n",
    "    '''\n",
    "    predictions: list of model predictions\n",
    "    references: corresponding list of test summaries\n",
    "    \n",
    "    returns: dictionary of rouge scores\n",
    "    '''\n",
    "    rouge = load_metric(\"rouge\")\n",
    "\n",
    "    # process text to make it compatible with rouge\n",
    "    predictions = [\" \".join(pred.strip().split()) for pred in predictions]\n",
    "    references = [\" \".join(ref.strip().split()) for ref in references]\n",
    "    predictions = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in predictions]\n",
    "    references = [\"\\n\".join(nltk.sent_tokenize(ref)) for ref in references]\n",
    "\n",
    "    # compute rouge scores\n",
    "    results = rouge.compute(\n",
    "        predictions=predictions,\n",
    "        references=references,\n",
    "        use_stemmer=True,\n",
    "        use_aggregator=False,\n",
    "    )\n",
    "    for key, value in results.items():\n",
    "        results[key] = {\n",
    "            \"precision\": [score.precision * 100 for score in value],\n",
    "            \"recall\": [score.recall * 100 for score in value],\n",
    "            \"fmeasure\": [score.fmeasure * 100 for score in value],\n",
    "            \"fmeasure_mean\": np.mean([score.fmeasure for score in value]) * 100,\n",
    "        }\n",
    "    # Compute the arithmetic mean of ROUGE-1, ROUGE-2 and ROUGE-L following: https://arxiv.org/abs/2110.08499\n",
    "    if all(rouge_type in results for rouge_type in [\"rouge1\", \"rouge2\", \"rougeL\"]):\n",
    "        results[\"rouge_avg_fmeasure\"] = np.mean(\n",
    "            [results[key][\"fmeasure\"] for key in [\"rouge1\", \"rouge2\", \"rougeL\"]], axis=0\n",
    "        ).tolist()\n",
    "        results[\"rouge_avg_fmeasure_mean\"] = np.mean(results[\"rouge_avg_fmeasure\"]).item()\n",
    "    else:\n",
    "        warnings.warn(\n",
    "            \"ROUGE-1, ROUGE-2 and ROUGE-L are not all present in the results. Skipping the computation of ROUGE-AVG.\"\n",
    "        )\n",
    "\n",
    "    return results\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bertscore(predictions, references, device):\n",
    "    '''\n",
    "    predictions: list of model predictions\n",
    "    references: corresponding list of test summaries\n",
    "    \n",
    "    returns: dictionary of bert scores\n",
    "    '''\n",
    "    bertscore = load_metric(\"bertscore\")\n",
    "\n",
    "    predictions = [\" \".join(pred.strip().split()) for pred in predictions]\n",
    "    references = [\" \".join(ref.strip().split()) for ref in references]\n",
    "    predictions = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in predictions]\n",
    "    references = [\"\\n\".join(nltk.sent_tokenize(ref)) for ref in references]\n",
    "\n",
    "    # Compute and post-process bertscore results\n",
    "    results = bertscore.compute(\n",
    "        predictions=predictions,\n",
    "        references=references,\n",
    "        # These are mostly based on the recommendations in https://github.com/Tiiiger/bert_score\n",
    "        model_type=BERTSCORE_MODEL_TYPE,\n",
    "        lang=\"en\",\n",
    "        rescale_with_baseline=True,\n",
    "        use_fast_tokenizer=True,\n",
    "        device = device\n",
    "    )\n",
    "    results[\"f1_mean\"] = np.mean(results[\"f1\"])\n",
    "    for key, value in results.items():\n",
    "        if key == \"hashcode\":\n",
    "            continue\n",
    "        if isinstance(value, list):\n",
    "            results[key] = [score * 100 for score in value]\n",
    "        else:\n",
    "            results[key] = value * 100\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_lengths(predictions, references):\n",
    "    pred_length = sum(len(pred) for pred in predictions)/len(predictions)\n",
    "    ref_length = sum(len(ref) for ref in references)/len(predictions)\n",
    "    return {'prediction': pred_length, 'reference': ref_length}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hallucination(nlp, predictions):\n",
    "    # returns percent of entities in generated impression not found in findings\n",
    "    unknown_words = {}\n",
    "    unknown_pcts = {}\n",
    "    total_unknown_pcts = 0\n",
    "    for i,pred in enumerate(predictions):\n",
    "        doc = nlp(pred)\n",
    "        unknown_words[i] = doc.ents\n",
    "        unknown_pct = len(doc.ents)/(len(set(pred.split()))+.0000000001)\n",
    "        unknown_pcts[i] = unknown_pct\n",
    "        total_unknown_pcts+= unknown_pct\n",
    "    avg_unknown_pct = total_unknown_pcts/(len(predictions)+.0000000001)\n",
    "    return unknown_words, unknown_pcts, avg_unknown_pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "    return logits.argmax(dim=-1)\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # rougeLSum expects newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    # Replace -100s used for padding as we can't decode them\n",
    "    preds = np.where(preds != -100, preds, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    # Otherwise rouge scores were a bit inflated\n",
    "    decoded_pred_radqas = [s.split('Answer:', 1)[-1].strip() if 'Answer:' in s else s for s in decoded_preds]\n",
    "    decoded_label_radqas = [s.split('Answer:', 1)[-1].strip() if 'Answer:' in s else s for s in decoded_labels]\n",
    "    print(\"decoded_preds:----------------------\\n\", decoded_pred_radqas[0:5])\n",
    "    print(\"decoded_labels:---------------------\\n\", decoded_label_radqas[0:5])\n",
    "\n",
    "    result = metric.compute(predictions=decoded_pred_radqas, references=decoded_label_radqas, use_stemmer=True)\n",
    "    result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"pred_len\"] = np.mean(prediction_lens)\n",
    "    reference_lens = [np.count_nonzero(label != tokenizer.pad_token_id) for label in labels]\n",
    "    result['ref_len'] = np.mean(reference_lens)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RadQA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'paragraphs': [{'qas': [{'question': 'Is ther...</td>\n",
       "      <td>full</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'paragraphs': [{'qas': [{'question': 'Does th...</td>\n",
       "      <td>full</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'paragraphs': [{'qas': [{'question': 'Is the ...</td>\n",
       "      <td>full</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'paragraphs': [{'qas': [{'question': 'Is an a...</td>\n",
       "      <td>full</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'paragraphs': [{'qas': [{'question': 'Is ther...</td>\n",
       "      <td>full</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>{'paragraphs': [{'qas': [{'question': 'Is ther...</td>\n",
       "      <td>full</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>{'paragraphs': [{'qas': [{'question': 'Is ther...</td>\n",
       "      <td>full</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>{'paragraphs': [{'qas': [{'question': 'Are the...</td>\n",
       "      <td>full</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>{'paragraphs': [{'qas': [{'question': 'Was the...</td>\n",
       "      <td>full</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>{'paragraphs': [{'qas': [{'question': 'Was per...</td>\n",
       "      <td>full</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>803 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  data version\n",
       "0    {'paragraphs': [{'qas': [{'question': 'Is ther...    full\n",
       "1    {'paragraphs': [{'qas': [{'question': 'Does th...    full\n",
       "2    {'paragraphs': [{'qas': [{'question': 'Is the ...    full\n",
       "3    {'paragraphs': [{'qas': [{'question': 'Is an a...    full\n",
       "4    {'paragraphs': [{'qas': [{'question': 'Is ther...    full\n",
       "..                                                 ...     ...\n",
       "798  {'paragraphs': [{'qas': [{'question': 'Is ther...    full\n",
       "799  {'paragraphs': [{'qas': [{'question': 'Is ther...    full\n",
       "800  {'paragraphs': [{'qas': [{'question': 'Are the...    full\n",
       "801  {'paragraphs': [{'qas': [{'question': 'Was the...    full\n",
       "802  {'paragraphs': [{'qas': [{'question': 'Was per...    full\n",
       "\n",
       "[803 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "radqa_train_df = pd.read_json('radqa/train.json')\n",
    "radqa_val_df = pd.read_json('radqa/dev.json')\n",
    "radqa_test_df = pd.read_json('radqa/test.json')\n",
    "radqa_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'paragraphs': [{'qas': [{'question': 'Is there any significant change in bleeding?',\n",
       "     'id': '796653_2_1_I',\n",
       "     'answers': [],\n",
       "     'is_impossible': True},\n",
       "    {'question': 'Did the bleeding in the sub-dural space resolve?',\n",
       "     'id': '796653_1_2_I',\n",
       "     'answers': [{'answer_id': '796653_1_2_I_MG',\n",
       "       'text': 'Subdural hematomas with blood products of different ages',\n",
       "       'answer_start': 13}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'Is there any additional bleeding in the sub-arachanoid space?',\n",
       "     'id': '796653_1_1_I',\n",
       "     'answers': [],\n",
       "     'is_impossible': True}],\n",
       "   'context': 'IMPRESSION:  Subdural hematomas with blood products of different ages.\\n Question vescular abnormality in left suprasellar space.  Findings were\\n discussed with Dr. [**Last Name (STitle) 8620**] at 9:25 am on [**2191-8-5**].  An MRI of the brain and MRA\\n of the COW is recommended.',\n",
       "   'document_id': '796653_I'},\n",
       "  {'qas': [{'question': 'Is there any additional bleeding in the sub-arachanoid space?',\n",
       "     'id': '796653_1_1_O',\n",
       "     'answers': [],\n",
       "     'is_impossible': True},\n",
       "    {'question': 'Did the bleeding in the sub-dural space resolve?',\n",
       "     'id': '796653_1_2_O',\n",
       "     'answers': [{'answer_id': '796653_1_2_O_MG',\n",
       "       'text': 'mixed density subdural hematomas seen along both cerebral\\n convexities, slightly larger on the left (approx 8-9mm) than on the right.\\n There is acute blood in the dependent parts of the subdural collections',\n",
       "       'answer_start': 757}],\n",
       "     'is_impossible': False},\n",
       "    {'question': 'Is there any significant change in bleeding?',\n",
       "     'id': '796653_2_1_O',\n",
       "     'answers': [{'answer_id': '796653_2_1_O_MG',\n",
       "       'text': 'no significant change',\n",
       "       'answer_start': 528}],\n",
       "     'is_impossible': False}],\n",
       "   'context': 'WET READ: MES FRI [**2191-8-5**] 1:40 AM\\n  no significant change in hemorrhage\\n ______________________________________________________________________________\\n                                 FINAL REPORT\\n INDICATION: known subarachnoid subdural hemorrhage from outside hospital.\\n Evaluate for any change.\\n\\n TECHNIQUE: Noncontrast head CT.\\n\\n COMPARISON: (CT done several hours earlier at [**Hospital 539**] Hospital).  At the time\\n of attending review, the prior exam is not available for comparison.\\n\\n FINDINGS: There has been no significant change in the interval. There is an\\n area of hyperdensity along the left anterior clinoid and in the adjacent\\n suprasellar space, which may be an aneurysm or small collection of blood, or a\\n dense mass.\\n There are mixed density subdural hematomas seen along both cerebral\\n convexities, slightly larger on the left (approx 8-9mm) than on the right.\\n There is acute blood in the dependent parts of the subdural collections. There\\n is flatteneing of the cerebral gyri and the lateral ventricles are small.\\n There is mild midline shift to the right, but the basal cisterns are not\\n narrowed at this time. [**Doctor Last Name **]/white matter differentiation is preserved. The\\n visualized paranasal sinuses and osseous structures are unremarkable.',\n",
       "   'document_id': '796653_O'}],\n",
       " 'title': '796653'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "radqa_train_df['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['full'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "radqa_train_df['version'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_radqa_df(df):\n",
    "    data_column = df['data']\n",
    "    normalized_data = pd.json_normalize(data_column, 'paragraphs', ['title'])\n",
    "    df_expanded = normalized_data.explode('qas').reset_index(drop=True)\n",
    "    df_expanded_qas = pd.json_normalize(df_expanded['qas']).add_prefix('qas.')\n",
    "    result_df = pd.concat([df_expanded, df_expanded_qas], axis=1)\n",
    "    result_df['answer_text'] = result_df['qas.answers'].apply(lambda x: x[0]['text'] if x else None)\n",
    "    result_df['answer_start'] = result_df['qas.answers'].apply(lambda x: str(x[0]['answer_start']) if x else 'Not in context.')\n",
    "    result_df = result_df.drop(columns=['qas', 'qas.answers'])\n",
    "    result_df['qas.answer'] = np.where(result_df['answer_text'],  result_df['answer_text'], 'Not in context.')\n",
    "#     result_df['qas.adjusted_answers'] = np.where(result_df['qas.is_impossible'] == True, 'Is impossible.', result_df['qas.answers'])\n",
    "    return result_df[['context', 'qas.question', 'qas.answer', 'answer_start']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>qas.question</th>\n",
       "      <th>qas.answer</th>\n",
       "      <th>answer_start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IMPRESSION:  Subdural hematomas with blood pro...</td>\n",
       "      <td>Is there any significant change in bleeding?</td>\n",
       "      <td>Not in context.</td>\n",
       "      <td>Not in context.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>IMPRESSION:  Subdural hematomas with blood pro...</td>\n",
       "      <td>Did the bleeding in the sub-dural space resolve?</td>\n",
       "      <td>Subdural hematomas with blood products of diff...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>IMPRESSION:  Subdural hematomas with blood pro...</td>\n",
       "      <td>Is there any additional bleeding in the sub-ar...</td>\n",
       "      <td>Not in context.</td>\n",
       "      <td>Not in context.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>WET READ: MES FRI [**2191-8-5**] 1:40 AM\\n  no...</td>\n",
       "      <td>Is there any additional bleeding in the sub-ar...</td>\n",
       "      <td>Not in context.</td>\n",
       "      <td>Not in context.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WET READ: MES FRI [**2191-8-5**] 1:40 AM\\n  no...</td>\n",
       "      <td>Did the bleeding in the sub-dural space resolve?</td>\n",
       "      <td>mixed density subdural hematomas seen along bo...</td>\n",
       "      <td>757</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  IMPRESSION:  Subdural hematomas with blood pro...   \n",
       "1  IMPRESSION:  Subdural hematomas with blood pro...   \n",
       "2  IMPRESSION:  Subdural hematomas with blood pro...   \n",
       "3  WET READ: MES FRI [**2191-8-5**] 1:40 AM\\n  no...   \n",
       "4  WET READ: MES FRI [**2191-8-5**] 1:40 AM\\n  no...   \n",
       "\n",
       "                                        qas.question  \\\n",
       "0       Is there any significant change in bleeding?   \n",
       "1   Did the bleeding in the sub-dural space resolve?   \n",
       "2  Is there any additional bleeding in the sub-ar...   \n",
       "3  Is there any additional bleeding in the sub-ar...   \n",
       "4   Did the bleeding in the sub-dural space resolve?   \n",
       "\n",
       "                                          qas.answer     answer_start  \n",
       "0                                    Not in context.  Not in context.  \n",
       "1  Subdural hematomas with blood products of diff...               13  \n",
       "2                                    Not in context.  Not in context.  \n",
       "3                                    Not in context.  Not in context.  \n",
       "4  mixed density subdural hematomas seen along bo...              757  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "radqa_clean_train_df = clean_radqa_df(radqa_train_df)\n",
    "radqa_clean_val_df = clean_radqa_df(radqa_val_df)\n",
    "radqa_clean_test_df = clean_radqa_df(radqa_test_df)\n",
    "radqa_clean_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WET READ: MES FRI [**2191-8-5**] 1:40 AM\\n  no significant change in hemorrhage\\n ______________________________________________________________________________\\n                                 FINAL REPORT\\n INDICATION: known subarachnoid subdural hemorrhage from outside hospital.\\n Evaluate for any change.\\n\\n TECHNIQUE: Noncontrast head CT.\\n\\n COMPARISON: (CT done several hours earlier at [**Hospital 539**] Hospital).  At the time\\n of attending review, the prior exam is not available for comparison.\\n\\n FINDINGS: There has been no significant change in the interval. There is an\\n area of hyperdensity along the left anterior clinoid and in the adjacent\\n suprasellar space, which may be an aneurysm or small collection of blood, or a\\n dense mass.\\n There are mixed density subdural hematomas seen along both cerebral\\n convexities, slightly larger on the left (approx 8-9mm) than on the right.\\n There is acute blood in the dependent parts of the subdural collections. There\\n is flatteneing of the cerebral gyri and the lateral ventricles are small.\\n There is mild midline shift to the right, but the basal cisterns are not\\n narrowed at this time. [**Doctor Last Name **]/white matter differentiation is preserved. The\\n visualized paranasal sinuses and osseous structures are unremarkable.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "radqa_clean_train_df['context'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is there any additional bleeding in the sub-arachanoid space?'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "radqa_clean_train_df['qas.question'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Not in context.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "radqa_clean_train_df['qas.answer'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "radqa_train_dataset = Dataset.from_pandas(radqa_clean_train_df)\n",
    "radqa_val_dataset = Dataset.from_pandas(radqa_clean_val_df)\n",
    "radqa_test_dataset = Dataset.from_pandas(radqa_clean_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['context', 'qas.question', 'qas.answer', 'answer_start'],\n",
       "    num_rows: 4878\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "radqa_train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt and inference set up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_radqa_prompt(example):\n",
    "    \"\"\"\n",
    "    params:\n",
    "    example: dataset with columns context, question, and answer\n",
    "    Ask model to return a sentence from the context that contains the answer\n",
    "    \n",
    "    returns:\n",
    "    list of prompts for each context-question-answer trio\n",
    "    \"\"\"\n",
    "    output_texts = []\n",
    "    for i in range(len(example['context'])):\n",
    "        text = f\"Context: {example['context'][i]}\\nQuestion: {example['qas.question'][i]}\\nWhat text from the context answers this question? If none, answer Not in context. Answer: {example['qas.answer'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_radqas(contexts, questions, model, tokenizer, max_response_length):\n",
    "    answers = []\n",
    "    for i, c in enumerate(tqdm(contexts)):\n",
    "#         prompt = f\"Context: {c}\\nQuestion: {questions[i]}\\nReturn 'Not in Context' if the answer is not in the context. Answer: \"\n",
    "#         prompt = f\"Context: {c}\\nQuestion: {questions[i]}\\nWhat word in the context does the answer start? Answer Start Position: \"\n",
    "        prompt = f\"Context: {c}\\nQuestion: {questions[i]}\\nWhat text from the context answers this question? If none, answer Not in context. Answer: \"\n",
    "#         max_length = len(prompt) + max_response_length\n",
    "        logits_process_list= LogitsProcessorList([EosTokenRewardLogitProcess(eos_token_id=tokenizer.eos_token_id, max_length=max_response_length)])\n",
    "        # add some postprocessor\n",
    "        pipe = pipeline(\n",
    "            task=\"text-generation\", \n",
    "            model=model, \n",
    "            tokenizer=tokenizer, \n",
    "            logits_processor=logits_process_list, \n",
    "            max_new_tokens=max_response_length, \n",
    "            return_full_text=False, \n",
    "            temperature=.1)\n",
    "        result = pipe(prompt)\n",
    "        answers.append(result[0]['generated_text'])\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = radqa_train_dataset['context']\n",
    "questions = radqa_train_dataset['qas.question']\n",
    "answers = radqa_train_dataset['qas.answer']\n",
    "answer_start = radqa_train_dataset['answer_start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:06<00:00,  1.23s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' No, there is no significant change in bleeding. The subdural hematoma is still present, and the blood products are of different ages.\\n',\n",
       " ' No.\\n',\n",
       " ' No.\\n',\n",
       " ' No.\\n',\n",
       " ' Not in Context.\\n']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = get_radqas(contexts[:5], questions[:5], model, tokenizer, 32)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Not in context.',\n",
       " 'Subdural hematomas with blood products of different ages',\n",
       " 'Not in context.',\n",
       " 'Not in context.',\n",
       " 'mixed density subdural hematomas seen along both cerebral\\n convexities, slightly larger on the left (approx 8-9mm) than on the right.\\n There is acute blood in the dependent parts of the subdural collections']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = get_radqas(contexts, questions, model, tokenizer, 30)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"radqa_predictions.json\"\n",
    "\n",
    "# Open the file in read mode and use json.load to read the list from the file\n",
    "with open(file_path, \"r\") as file:\n",
    "    predictions = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84.44485444854449"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(answers[i]) for i in range(len(answers))])/len(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_902807/3726123131.py:8: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge1: 19.492784398961184\n",
      "Rouge2: 8.87716980178776\n",
      "RougeL: 17.73556326141762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert score: 7.582608859104075\n",
      "Average response lengths: {'prediction': 79.12136121361213, 'reference': 84.44485444854449}\n"
     ]
    }
   ],
   "source": [
    "rouge_scores = get_rouge_scores(predictions, answers)\n",
    "print(f\"Rouge1: {rouge_scores['rouge1']['fmeasure_mean']}\")\n",
    "print(f\"Rouge2: {rouge_scores['rouge2']['fmeasure_mean']}\")\n",
    "print(f\"RougeL: {rouge_scores['rougeL']['fmeasure_mean']}\")\n",
    "bert_scores = get_bertscore(predictions, answers)\n",
    "print(f\"Bert score: {bert_scores['f1_mean']}\")\n",
    "avg_response_lengths = compare_lengths(predictions, answers)\n",
    "print(f\"Average response lengths: {avg_response_lengths}\")\n",
    "# base_spacy_scores = test_hallucination(nlp, predictions)\n",
    "# print(f\"Hallucination percent: {base_spacy_scores[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "f1 = f1_score(answers, predictions, average='weighted')\n",
    "\n",
    "# Print the F1 score\n",
    "print(f'F1 Score: {f1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt to get answer start position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops F1 score of 0 makes sense because it's not meant for text to text models. We will adjust the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:04<00:00,  1.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['10\\n', '10\\n\\n', '10\\n', '2191-8-5\\n', '2191-8-5\\n']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = get_radqas(contexts[:5], questions[:5], model, tokenizer, 32)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Not in context.', '13', 'Not in context.', 'Not in context.', '757']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_start[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt to get answer form context - promising compromise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [22:49<00:00,  2.74s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' Yes, there is a significant change in bleeding.\\n',\n",
       " ' Not in context. The text does not provide any information about the resolution of the bleeding in the sub-dural space.\\n\\nAnswer: Not in',\n",
       " ' Yes, there is additional bleeding in the sub-arachnoid space.\\n\\n',\n",
       " ' Not in context.\\n\\n',\n",
       " ' Not in context. There is no text in the provided context that directly answers the question of whether the bleeding in the sub-dural space resolved. The',\n",
       " ' Not in context. There is no text in the provided context that directly answers the question about significant change in bleeding. The report only mentions the initial finding of',\n",
       " '2) There is contrast material within the kidneys; this may represent ATN or continued renal excretion of orally administered contrast.',\n",
       " '2) There is contrast material within the kidneys; this may represent ATN or continued renal excretion of orally administered contrast.',\n",
       " '2) There is contrast material within the kidneys; this may represent ATN or continued renal excretion of orally administered contrast.',\n",
       " ' There is high attenuation within the kidneys bilaterally, consistent with either ATN or related to enteric oral contrast absorption and']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = get_radqas(contexts[:500], questions[:500], model, tokenizer, 32)\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Not in context.',\n",
       " 'Subdural hematomas with blood products of different ages',\n",
       " 'Not in context.',\n",
       " 'Not in context.',\n",
       " 'mixed density subdural hematomas seen along both cerebral\\n convexities, slightly larger on the left (approx 8-9mm) than on the right.\\n There is acute blood in the dependent parts of the subdural collections',\n",
       " 'no significant change',\n",
       " 'obstruction at the area of ileal anastomosis',\n",
       " 'Not in context.',\n",
       " 'contrast material within the kidneys; this may represent ATN or\\n continued renal excretion of orally administered contrast',\n",
       " 'high attenuation  within the kidneys bilaterally, consistent with\\n either ATN or related to enteric oral contrast absorption and excretion\\n continually by the kidneys due to obstruction']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge1: 21.317079660752867\n",
      "Rouge2: 14.844763851352388\n",
      "RougeL: 20.568796939284567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert score: 6.075675392569974\n",
      "Average response lengths: {'prediction': 71.112, 'reference': 59.504}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    }
   ],
   "source": [
    "rouge_scores = get_rouge_scores(predictions, answers[:500])\n",
    "print(f\"Rouge1: {rouge_scores['rouge1']['fmeasure_mean']}\")\n",
    "print(f\"Rouge2: {rouge_scores['rouge2']['fmeasure_mean']}\")\n",
    "print(f\"RougeL: {rouge_scores['rougeL']['fmeasure_mean']}\")\n",
    "bert_scores = get_bertscore(predictions, answers[:500], 7)\n",
    "print(f\"Bert score: {bert_scores['f1_mean']}\")\n",
    "avg_response_lengths = compare_lengths(predictions, answers[:500])\n",
    "print(f\"Average response lengths: {avg_response_lengths}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference on radiology model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/imx2/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "debc7719cf6e4bf79c1194d0e5a005fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/633 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1745c6c79b1447539f9f1274301008d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5179633a6a0a48b1a78176812ea2163f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b30d89fd7564588b3e6335ec9fe3634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d539980328b4ae79127f2e9c15f0dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a515f0567db248efbc47ceac51ad973f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4307625fade745a5b4baf32d97a74be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"imxx/llama-2-7b-chest-pelvis-mri-pelvis\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [14:28<00:00,  1.74s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' No',\n",
       " ' No',\n",
       " ' No',\n",
       " ' No.\\n\\n The critical findings described above were communicated via the Veriphy Critical Results Reporting System as a Yellow critical result.  ',\n",
       " ' No.\\n\\n The critical findings described above were communicated via the Veriphy Critical Results Reporting System as a Yellow critical result.  ',\n",
       " ' No.\\n The critical findings described above were communicated via the Veriphy Critical Results Reporting System as a Yellow critical result.  \\n',\n",
       " '1. Small bowel obstruction at the area of ileal anastomosis. 2. Focal bowel wall thickening and hypermet',\n",
       " '1. Small bowel obstruction at the area of ileal anastomosis. 2. Fecal obstruction.  \\n Given this',\n",
       " ' No',\n",
       " ' No.\\n\\n']"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = get_radqas(contexts[:500], questions[:500], model, tokenizer, 32)\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results don't seem very promising :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge1: 12.411193973482773\n",
      "Rouge2: 6.132884068738024\n",
      "RougeL: 12.338329193119382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert score: -12.673906405025628\n",
      "Average response lengths: {'prediction': 45.058, 'reference': 59.504}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    }
   ],
   "source": [
    "rouge_scores = get_rouge_scores(predictions, answers[:500])\n",
    "print(f\"Rouge1: {rouge_scores['rouge1']['fmeasure_mean']}\")\n",
    "print(f\"Rouge2: {rouge_scores['rouge2']['fmeasure_mean']}\")\n",
    "print(f\"RougeL: {rouge_scores['rougeL']['fmeasure_mean']}\")\n",
    "bert_scores = get_bertscore(predictions, answers[:500], 7)\n",
    "print(f\"Bert score: {bert_scores['f1_mean']}\")\n",
    "avg_response_lengths = compare_lengths(predictions, answers[:500])\n",
    "print(f\"Average response lengths: {avg_response_lengths}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune RadQA on base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3e67cb16b74a4f93988bd95a14d831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "new_model = \"llama-2-7b-radnlp-radqa\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Evidence of overfitting, so we increase lora dropout and rerun the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imx2/miniconda3/envs/myenv/lib/python3.8/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acaea2eafa8d43a081397210bcef2345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4878 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b8c2c9af26645a4b5b9ce3cc5597e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/656 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/imx2/miniconda3/envs/myenv/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6100' max='6100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6100/6100 2:00:14, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Pred Len</th>\n",
       "      <th>Ref Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.866700</td>\n",
       "      <td>1.161797</td>\n",
       "      <td>57.063000</td>\n",
       "      <td>51.770100</td>\n",
       "      <td>57.112900</td>\n",
       "      <td>57.121200</td>\n",
       "      <td>335.585366</td>\n",
       "      <td>190.600610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.852300</td>\n",
       "      <td>1.103447</td>\n",
       "      <td>57.986800</td>\n",
       "      <td>52.534700</td>\n",
       "      <td>58.017200</td>\n",
       "      <td>58.062900</td>\n",
       "      <td>335.585366</td>\n",
       "      <td>190.600610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.896000</td>\n",
       "      <td>1.081809</td>\n",
       "      <td>74.909600</td>\n",
       "      <td>68.940600</td>\n",
       "      <td>74.859600</td>\n",
       "      <td>74.940300</td>\n",
       "      <td>335.585366</td>\n",
       "      <td>190.600610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1.103495</td>\n",
       "      <td>64.432300</td>\n",
       "      <td>59.086100</td>\n",
       "      <td>64.474500</td>\n",
       "      <td>64.591700</td>\n",
       "      <td>335.585366</td>\n",
       "      <td>190.600610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.585400</td>\n",
       "      <td>1.142377</td>\n",
       "      <td>64.979800</td>\n",
       "      <td>59.770100</td>\n",
       "      <td>65.004000</td>\n",
       "      <td>65.070500</td>\n",
       "      <td>335.585366</td>\n",
       "      <td>190.600610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.550300</td>\n",
       "      <td>1.187377</td>\n",
       "      <td>67.094400</td>\n",
       "      <td>61.412500</td>\n",
       "      <td>67.124200</td>\n",
       "      <td>67.233700</td>\n",
       "      <td>335.585366</td>\n",
       "      <td>190.600610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.554200</td>\n",
       "      <td>1.211889</td>\n",
       "      <td>73.389800</td>\n",
       "      <td>67.176800</td>\n",
       "      <td>73.377800</td>\n",
       "      <td>73.411300</td>\n",
       "      <td>335.585366</td>\n",
       "      <td>190.600610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.386500</td>\n",
       "      <td>1.266108</td>\n",
       "      <td>77.048800</td>\n",
       "      <td>71.267600</td>\n",
       "      <td>77.064200</td>\n",
       "      <td>77.096700</td>\n",
       "      <td>335.585366</td>\n",
       "      <td>190.600610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.320300</td>\n",
       "      <td>1.316458</td>\n",
       "      <td>76.313300</td>\n",
       "      <td>70.138600</td>\n",
       "      <td>76.316000</td>\n",
       "      <td>76.336400</td>\n",
       "      <td>335.585366</td>\n",
       "      <td>190.600610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.427700</td>\n",
       "      <td>1.348567</td>\n",
       "      <td>73.846600</td>\n",
       "      <td>67.627700</td>\n",
       "      <td>73.822300</td>\n",
       "      <td>73.864900</td>\n",
       "      <td>335.585366</td>\n",
       "      <td>190.600610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.390600</td>\n",
       "      <td>1.364944</td>\n",
       "      <td>76.039700</td>\n",
       "      <td>69.802100</td>\n",
       "      <td>76.024800</td>\n",
       "      <td>76.059600</td>\n",
       "      <td>335.585366</td>\n",
       "      <td>190.600610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.399800</td>\n",
       "      <td>1.370894</td>\n",
       "      <td>75.873400</td>\n",
       "      <td>69.632200</td>\n",
       "      <td>75.865000</td>\n",
       "      <td>75.882000</td>\n",
       "      <td>335.585366</td>\n",
       "      <td>190.600610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded_preds:----------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia.\\nin in in in in concerning concerning in in in in concerning concerning concerning concerning concerning in in is is concerning concerning concerning concerning concerning concerning concerning concerning in in in in in in in in in answer answer question in in in question answer question in', 'Not mid lung opacity is in is::: is is:::::::::::::::::::::::: is as concerning in in', 'small right mid-lung\\n opacity, concerning for pneumonia.', 'small right mid-lung\\n opacity,\\n\\n question question question question question question question question', 'no in context.\\n[']\n",
      "decoded_labels:---------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia', 'Right mid lung opacity', 'small right mid-lung\\n opacity, concerning for pneumonia', 'small right mid-lung\\n opacity', 'Not in context.']\n",
      "decoded_preds:----------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia.\\nin in in in in in in in in in in in in in in in in in in in in', 'Right mid lung opacity is in in in in in in in in in in in: as as as as as as as as as', 'small right mid-lung\\n opacity, concerning for pneumonia.', 'small right mid-lung\\n opacity,\\n\\n\\n\\n Question Question', 'No in context.']\n",
      "decoded_labels:---------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia', 'Right mid lung opacity', 'small right mid-lung\\n opacity, concerning for pneumonia', 'small right mid-lung\\n opacity', 'Not in context.']\n",
      "decoded_preds:----------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia.\\nin in in in in in in in in in in in in in', 'Right mid lung opacity is in in in possibility in in in09 and and and in in in in in', 'small right mid-lung\\n opacity, concerning for pneumonia.', 'small right mid-lung\\n opacity,', 'No in context.']\n",
      "decoded_labels:---------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia', 'Right mid lung opacity', 'small right mid-lung\\n opacity, concerning for pneumonia', 'small right mid-lung\\n opacity', 'Not in context.']\n",
      "decoded_preds:----------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia.\\nin as as as in', 'Right mid lung opacity is in in in in in in in in in in in as in in', 'small right mid-lung\\n opacity, concerning for pneumonia.', 'small right mid-lung\\n opacity,', 'Not in context.']\n",
      "decoded_labels:---------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia', 'Right mid lung opacity', 'small right mid-lung\\n opacity, concerning for pneumonia', 'small right mid-lung\\n opacity', 'Not in context.']\n",
      "decoded_preds:----------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia.\\nin in report in cause cause', 'Not mid lung opacity is in in in in in in in in in in in in in in in in in in in in', 'small right mid-lung\\n opacity, concerning for pneumonia.', 'small right mid-lung\\n opacity,', 'Not in context.']\n",
      "decoded_labels:---------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia', 'Right mid lung opacity', 'small right mid-lung\\n opacity, concerning for pneumonia', 'small right mid-lung\\n opacity', 'Not in context.']\n",
      "decoded_preds:----------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia.\\nas as asinkinkinkink in', 'Not mid lung opacity is in in in in in in in in in in in in in in in in in in in', 'small right mid-lung\\n opacity, concerning for pneumonia.', 'small right mid-lung\\n opacity,', 'Lar in context.']\n",
      "decoded_labels:---------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia', 'Right mid lung opacity', 'small right mid-lung\\n opacity, concerning for pneumonia', 'small right mid-lung\\n opacity', 'Not in context.']\n",
      "decoded_preds:----------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia.', 'Right mid lung opacity is in in in in in in in in in in in', 'small right mid-lung\\n opacity, concerning for pneumonia.', 'small right mid-lung\\n opacity,', 'Lar in context.']\n",
      "decoded_labels:---------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia', 'Right mid lung opacity', 'small right mid-lung\\n opacity, concerning for pneumonia', 'small right mid-lung\\n opacity', 'Not in context.']\n",
      "decoded_preds:----------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia.', 'Not mid lung opacity is in in in in in in in in in in in', 'small right mid-lung\\n opacity, concerning for pneumonia.', 'Not right mid-lung\\n opacity,', 'Not in context.']\n",
      "decoded_labels:---------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia', 'Right mid lung opacity', 'small right mid-lung\\n opacity, concerning for pneumonia', 'small right mid-lung\\n opacity', 'Not in context.']\n",
      "decoded_preds:----------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia.', 'Right mid lung opacity is in in in in in in in as as as in', 'small right mid-lung\\n opacity, concerning for pneumonia.', 'Not right mid-lung\\n opacity,', 'Lar in context.']\n",
      "decoded_labels:---------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia', 'Right mid lung opacity', 'small right mid-lung\\n opacity, concerning for pneumonia', 'small right mid-lung\\n opacity', 'Not in context.']\n",
      "decoded_preds:----------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia.', 'Right mid lung opacity is in in in in in in as as as as in', 'small right mid-lung\\n opacity, concerning for pneumonia.', 'Not right mid-lung\\n opacity,', 'Lar in context.']\n",
      "decoded_labels:---------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia', 'Right mid lung opacity', 'small right mid-lung\\n opacity, concerning for pneumonia', 'small right mid-lung\\n opacity', 'Not in context.']\n",
      "decoded_preds:----------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia.', 'Right mid lung opacity is in in in in in in in as as as', 'small right mid-lung\\n opacity, concerning for pneumonia.', 'small right mid-lung\\n opacity,', 'Lar in context.']\n",
      "decoded_labels:---------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia', 'Right mid lung opacity', 'small right mid-lung\\n opacity, concerning for pneumonia', 'small right mid-lung\\n opacity', 'Not in context.']\n",
      "decoded_preds:----------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia.', 'Right mid lung opacity is in in in in in in in as as as', 'small right mid-lung\\n opacity, concerning for pneumonia.', 'small right mid-lung\\n opacity,', 'Lar in context.']\n",
      "decoded_labels:---------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia', 'Right mid lung opacity', 'small right mid-lung\\n opacity, concerning for pneumonia', 'small right mid-lung\\n opacity', 'Not in context.']\n"
     ]
    }
   ],
   "source": [
    "finetune(model, \n",
    "         radqa_train_dataset, #ct_impressions_train_dataset, \n",
    "         radqa_val_dataset,\n",
    "         peft_config, \n",
    "         1024, \n",
    "         tokenizer, \n",
    "         training_arguments, \n",
    "         packing, \n",
    "         generate_radqa_prompt, \n",
    "         new_model,\n",
    "         compute_metrics,\n",
    "         preprocess_logits_for_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment: increase dropout rate to .25 and reduce number of epochs to 3 to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42256cbe24c2466d90dad28049a4a56d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4878 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "580050c181604ab796ce24092623e7b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/656 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3660' max='3660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3660/3660 1:11:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Pred Len</th>\n",
       "      <th>Ref Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.862600</td>\n",
       "      <td>1.152484</td>\n",
       "      <td>52.958300</td>\n",
       "      <td>48.700400</td>\n",
       "      <td>52.911800</td>\n",
       "      <td>52.960000</td>\n",
       "      <td>335.585366</td>\n",
       "      <td>190.600610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.846900</td>\n",
       "      <td>1.096629</td>\n",
       "      <td>51.282000</td>\n",
       "      <td>46.628200</td>\n",
       "      <td>51.313300</td>\n",
       "      <td>51.364200</td>\n",
       "      <td>335.585366</td>\n",
       "      <td>190.600610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.894800</td>\n",
       "      <td>1.077609</td>\n",
       "      <td>55.082100</td>\n",
       "      <td>50.433900</td>\n",
       "      <td>55.072500</td>\n",
       "      <td>55.199900</td>\n",
       "      <td>335.585366</td>\n",
       "      <td>190.600610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.881700</td>\n",
       "      <td>1.083837</td>\n",
       "      <td>54.995900</td>\n",
       "      <td>50.268000</td>\n",
       "      <td>54.983600</td>\n",
       "      <td>55.021300</td>\n",
       "      <td>335.585366</td>\n",
       "      <td>190.600610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.610200</td>\n",
       "      <td>1.124918</td>\n",
       "      <td>55.273800</td>\n",
       "      <td>50.905700</td>\n",
       "      <td>55.239600</td>\n",
       "      <td>55.339900</td>\n",
       "      <td>335.585366</td>\n",
       "      <td>190.600610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.585200</td>\n",
       "      <td>1.135888</td>\n",
       "      <td>55.648700</td>\n",
       "      <td>50.865000</td>\n",
       "      <td>55.574400</td>\n",
       "      <td>55.690500</td>\n",
       "      <td>335.585366</td>\n",
       "      <td>190.600610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.626500</td>\n",
       "      <td>1.137062</td>\n",
       "      <td>55.936200</td>\n",
       "      <td>51.320200</td>\n",
       "      <td>55.854400</td>\n",
       "      <td>55.962400</td>\n",
       "      <td>335.585366</td>\n",
       "      <td>190.600610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded_preds:----------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia.\\nin in in in in in00 in in in in0000\\n\\n0000 in in in in in in in in in in in in in in in context context context in in in question question question in in in in', 'Not mid lung opacity is in9:::99::::::::::::::::::::::::::: in context context in context', 'small right mid-lung\\n opacity, concerning for pneumonia.', 'small right mid-lung\\n opacity,\\n question question question question question question question question', 'Not in context.']\n",
      "decoded_labels:---------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia', 'Right mid lung opacity', 'small right mid-lung\\n opacity, concerning for pneumonia', 'small right mid-lung\\n opacity', 'Not in context.']\n",
      "decoded_preds:----------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia.\\nin in in in in in in in in in in in in in in in in in in in', 'Right mid lung opacity is in in in in in:::::::::: in in in: in', 'small right mid-lung\\n opacity, concerning for pneumonia.', 'small right mid-lung\\n opacity,\\n\\n\\n question question question question', 'No in context.\\nAnswer']\n",
      "decoded_labels:---------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia', 'Right mid lung opacity', 'small right mid-lung\\n opacity, concerning for pneumonia', 'small right mid-lung\\n opacity', 'Not in context.']\n",
      "decoded_preds:----------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia.\\nin in in in in in in in in in in in in in in', 'Right mid lung opacity is in in in in::::::00:: in in in in in', 'small right mid-lung\\n opacity, concerning for pneumonia.', 'small right mid-lung\\n opacity, N', 'Not in context.']\n",
      "decoded_labels:---------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia', 'Right mid lung opacity', 'small right mid-lung\\n opacity, concerning for pneumonia', 'small right mid-lung\\n opacity', 'Not in context.']\n",
      "decoded_preds:----------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia.', 'Right mid lung opacity is Context in in in in in in: in in', 'small right mid-lung\\n opacity, concerning for pneumonia.', 'small right mid-lung\\n opacity,', 'Not in context.\\nAnswer']\n",
      "decoded_labels:---------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia', 'Right mid lung opacity', 'small right mid-lung\\n opacity, concerning for pneumonia', 'small right mid-lung\\n opacity', 'Not in context.']\n",
      "decoded_preds:----------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia.\\nin in in in in in in', 'Not mid lung opacity is in in questions questions questions in in in questions questions questions in in in in in', 'small right mid-lung\\n opacity, concerning for pneumonia.', 'Not right mid-lung\\n opacity,', 'Not in context.']\n",
      "decoded_labels:---------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia', 'Right mid lung opacity', 'small right mid-lung\\n opacity, concerning for pneumonia', 'small right mid-lung\\n opacity', 'Not in context.']\n",
      "decoded_preds:----------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia.\\nin in in', 'Right mid lung opacity is Context Context in in in in questions questions questions questions questions questions questions questions questions: in in in in in', 'small right mid-lung\\n opacity, concerning for pneumonia.', 'small right mid-lung\\n opacity,', 'Not in context.\\nAnswer']\n",
      "decoded_labels:---------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia', 'Right mid lung opacity', 'small right mid-lung\\n opacity, concerning for pneumonia', 'small right mid-lung\\n opacity', 'Not in context.']\n",
      "decoded_preds:----------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia.', 'Right mid lung opacity is Context Context in in questions questions questions questions in questions questions questions questions in in in in Context', 'small right mid-lung\\n opacity, concerning for pneumonia.', 'small right mid-lung\\n opacity,', 'Not in context.\\nAnswer']\n",
      "decoded_labels:---------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia', 'Right mid lung opacity', 'small right mid-lung\\n opacity, concerning for pneumonia', 'small right mid-lung\\n opacity', 'Not in context.']\n"
     ]
    }
   ],
   "source": [
    "finetune(model, \n",
    "         radqa_train_dataset, #ct_impressions_train_dataset, \n",
    "         radqa_val_dataset,\n",
    "         peft_config, \n",
    "         1024, \n",
    "         tokenizer, \n",
    "         training_arguments, \n",
    "         packing, \n",
    "         generate_radqa_prompt, \n",
    "         new_model,\n",
    "         compute_metrics,\n",
    "         preprocess_logits_for_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ceb1bed515f4f8282230c8c78aff9b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/imx2/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3966abefb524418abcc843b5dde0eb6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b223e545944a44e2ba1ac0cadbbb13f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36be05fee41e47c385cd76ad7eb82a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/imxx/llama-2-7b-radnlp-radqa/commit/e427c4b68a2766c0f3e9a6d9bd2aa648da4acfbe', commit_message='Upload tokenizer', commit_description='', oid='e427c4b68a2766c0f3e9a6d9bd2aa648da4acfbe', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!huggingface-cli login --token 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\n",
    "\n",
    "model.push_to_hub(new_model, use_temp_dir=False)\n",
    "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57bcb1dd2fe74ed5a0b4201913bf6dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/630 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4247718fbe584213b4da3df73d89941d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3c2e5612c44c909f1c1e93c616a9a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "024e3666bf0144a89f653660cfa5a367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2f1a749b86406b83530e1b1136d6f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf03e6e9fe049a1bd810108f8f874c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0909281a98f47deba0ada37b6612556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"imxx/llama-2-7b-radnlp-radqa\"\n",
    "new_model = \"llama-2-7b-radnlp-chest-pelvis-mri-petct-radqa\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                        | 0/500 [00:00<?, ?it/s]/home/imx2/miniconda3/envs/myenv/lib/python3.8/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [29:27<00:00,  3.53s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' Patent celiac and SMA, there is no ultrasound evidence of\\n stenosis. [**Female First Name (un',\n",
       " ' Patent celiac and SMA, there is no ultrasound evidence of\\n stenosis. [**Female First Name (un',\n",
       " ' Patent celiac and SMA, there is no ultrasound evidence of\\n stenosis. [**Female First Name (un',\n",
       " ' the celiac and SMA are widely patent, Doppler\\n assessment shows normal spectral flow and velocities within normal limits.\\n There',\n",
       " ' the celiac and SMA are widely patent, Doppler\\n assessment shows normal spectral flow and velocities within normal limits.\\n There',\n",
       " ' the celiac and SMA are widely patent, Doppler\\n assessment shows normal spectral flow and velocities within normal limits.\\n There',\n",
       " '1) ET tube tip in good position, 2 cm above the carina.\\n\\n 2) Nasogatric tube',\n",
       " '1) ET tube tip in good position, 2 cm above the carina.\\n\\n 2) Nasogatric tube',\n",
       " '2 cm above the carina.  There is no\\n change in the position of the left IJ central venous catheter; the nas',\n",
       " '2 cm above the carina.  There is no\\n change in the position of the left IJ central venous catheter; the nas']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts = radqa_test_dataset['context']\n",
    "questions = radqa_test_dataset['qas.question']\n",
    "answers = radqa_test_dataset['qas.answer']\n",
    "predictions = get_radqas(contexts[:500], questions[:500], model, tokenizer, 30)\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no ultrasound evidence of\\n stenosis',\n",
       " 'no ultrasound evidence of\\n stenosis',\n",
       " 'Patent celiac and SMA',\n",
       " 'SMA are widely patent',\n",
       " 'SMA are widely patent',\n",
       " 'Doppler\\n assessment shows normal spectral flow and velocities within normal limits',\n",
       " 'tube tip in good position, 2 cm above the carina',\n",
       " '2 cm above the carina',\n",
       " '2 cm above the carina',\n",
       " 'tube tip is in satisfactory position, 2 cm above the carina']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge1: 36.097263516484794\n",
      "Rouge2: 30.084301002803933\n",
      "RougeL: 35.193033008240356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert score: 22.836305712512694\n",
      "Average response lengths: {'prediction': 110.63, 'reference': 55.984}\n"
     ]
    }
   ],
   "source": [
    "rouge_scores = get_rouge_scores(predictions, answers[:500])\n",
    "print(f\"Rouge1: {rouge_scores['rouge1']['fmeasure_mean']}\")\n",
    "print(f\"Rouge2: {rouge_scores['rouge2']['fmeasure_mean']}\")\n",
    "print(f\"RougeL: {rouge_scores['rougeL']['fmeasure_mean']}\")\n",
    "bert_scores = get_bertscore(predictions, answers[:500], 7)\n",
    "print(f\"Bert score: {bert_scores['f1_mean']}\")\n",
    "avg_response_lengths = compare_lengths(predictions, answers[:500])\n",
    "print(f\"Average response lengths: {avg_response_lengths}\")\n",
    "# base_spacy_scores = test_hallucination(nlp, predictions)\n",
    "# print(f\"Hallucination percent: {base_spacy_scores[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune on general model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2837143e2bfb45df875526e7b50faca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"imxx/llama-2-7b-chest-pelvis-mri-pelvis\"\n",
    "new_model = \"llama-2-7b-radnlp-chest-pelvis-mri-petct-radqa\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imx2/miniconda3/envs/myenv/lib/python3.8/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de92500d6ea7470f8ae0672198ee73a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4878 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4271e2fd85914c55818efc31bac53187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/656 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/imx2/miniconda3/envs/myenv/lib/python3.8/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3660' max='3660' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3660/3660 1:12:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Pred Len</th>\n",
       "      <th>Ref Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.840700</td>\n",
       "      <td>1.115452</td>\n",
       "      <td>52.092200</td>\n",
       "      <td>47.387500</td>\n",
       "      <td>52.194000</td>\n",
       "      <td>52.261900</td>\n",
       "      <td>335.585366</td>\n",
       "      <td>190.600610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.831700</td>\n",
       "      <td>1.075856</td>\n",
       "      <td>52.591600</td>\n",
       "      <td>48.757300</td>\n",
       "      <td>52.640600</td>\n",
       "      <td>52.733100</td>\n",
       "      <td>335.585366</td>\n",
       "      <td>190.600610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.866500</td>\n",
       "      <td>1.072088</td>\n",
       "      <td>53.806700</td>\n",
       "      <td>49.277100</td>\n",
       "      <td>53.829400</td>\n",
       "      <td>53.941900</td>\n",
       "      <td>335.585366</td>\n",
       "      <td>190.600610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.859300</td>\n",
       "      <td>1.079270</td>\n",
       "      <td>55.737700</td>\n",
       "      <td>51.478400</td>\n",
       "      <td>55.743100</td>\n",
       "      <td>55.939800</td>\n",
       "      <td>335.585366</td>\n",
       "      <td>190.600610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.598500</td>\n",
       "      <td>1.111609</td>\n",
       "      <td>57.181500</td>\n",
       "      <td>52.466100</td>\n",
       "      <td>57.134800</td>\n",
       "      <td>57.314800</td>\n",
       "      <td>335.585366</td>\n",
       "      <td>190.600610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.573500</td>\n",
       "      <td>1.134341</td>\n",
       "      <td>56.438200</td>\n",
       "      <td>51.648600</td>\n",
       "      <td>56.385300</td>\n",
       "      <td>56.587200</td>\n",
       "      <td>335.585366</td>\n",
       "      <td>190.600610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.609000</td>\n",
       "      <td>1.133206</td>\n",
       "      <td>56.973200</td>\n",
       "      <td>52.397600</td>\n",
       "      <td>56.926700</td>\n",
       "      <td>57.115400</td>\n",
       "      <td>335.585366</td>\n",
       "      <td>190.600610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoded_preds:----------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia.\\nin in in in in\\n\\n in in in\\n\\n\\n and and and and\\n\\n\\n text text text text text text ch ch ch ch text ch ch ch ch ch ch ch ch text text text text text text text text question in question question question question question question question question question question question question question question question', 'Not mid lung opacity is Context Context Context Context Context Context Context Context Context Context Context Context Context Context Context and and and and and is is is text text is,,::::::::::::::QuestionQuestionQuestion:::: and and and text: text text text text text question question questionlylylylyly Context Context Context Context Context Context Context Context Context Context Context Context Context Context Context Context Context Context Context Context Context', 'small right mid-lung\\n opacity, concerning for pneumonia.', 'small right mid-lung\\n opacity,', 'Lar in context.']\n",
      "decoded_labels:---------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia', 'Right mid lung opacity', 'small right mid-lung\\n opacity, concerning for pneumonia', 'small right mid-lung\\n opacity', 'Not in context.']\n",
      "decoded_preds:----------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia.\\nin in in in in in in in in in inly in in in in textly in in in innessnesslyly innessnesslyly Context Context Context Context Context Context Context Context Context Context Context Context Context Context Context Context Context Context Context', 'Right mid lung opacity is Context Context Context Context Context Context Context as in in Context Context as as in in in in in in in inm in in in in text text in innessnessnessness in inlyly Context Context Context Context Context Context Context Context Context Context Context Context Context Context Context Context Context Context Context Context Context Context Context', 'small right mid-lung\\n opacity, concerning for pneumonia.', 'no right mid-lung\\n opacity,', 'Not in context.']\n",
      "decoded_labels:---------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia', 'Right mid lung opacity', 'small right mid-lung\\n opacity, concerning for pneumonia', 'small right mid-lung\\n opacity', 'Not in context.']\n",
      "decoded_preds:----------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia.\\nin in in in in in in in in in in imag imag in in inh in in in in in in in in in imag imag imag in in ch imagly textnessness chlylylyly Context Context Context Context', 'Right mid lung opacity is Context Context Context Context Context Context as and in in in and and\\n in in imag improved\\n\\n:://\\n---------- answered answered in mentioned mentioned text Context Context Context Context Context Context Context Context', 'small right mid-lung\\n opacity, concerning for pneumonia.', 'no right mid-lung\\n opacity,', 'Not in context.']\n",
      "decoded_labels:---------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia', 'Right mid lung opacity', 'small right mid-lung\\n opacity, concerning for pneumonia', 'small right mid-lung\\n opacity', 'Not in context.']\n",
      "decoded_preds:----------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia.\\nin inness imag in in in in in in in in imag imag implly imag imaglylylylylylyly', 'Right mid lung opacity is Context Context Context Context Context Context Context was is was Context Context was wasnessmmmmmmmmmmmmmmmmmmmm is was wasly Context Context Context Context Context Context', 'small right mid-lung\\n opacity, concerning for pneumonia.', 'small right mid-lung\\n opacity,', 'Not in context.\\nA']\n",
      "decoded_labels:---------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia', 'Right mid lung opacity', 'small right mid-lung\\n opacity, concerning for pneumonia', 'small right mid-lung\\n opacity', 'Not in context.']\n",
      "decoded_preds:----------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia.\\nimpl impl impl implips impl impl impl impl imag imag impl imag imag impllyly', 'Right mid lung opacity is Context Context Context Context Context Context Context improved improved Context Context Context/ improved improvedmm improved Im imagmmmmmmmmm im im shown shown imm shown are Context Context Context Context Context Context', 'small right mid-lung\\n opacity, concerning for pneumonia.', 'small right mid-lung\\n opacity,', 'Not in context.']\n",
      "decoded_labels:---------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia', 'Right mid lung opacity', 'small right mid-lung\\n opacity, concerning for pneumonia', 'small right mid-lung\\n opacity', 'Not in context.']\n",
      "decoded_preds:----------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia.\\nin impl impl impl impl impl in impl impl impl impl imag impl in imag imag impl B in imag imag in', 'Right mid lung opacity is Context Context Context Context Context Context are are improved are Context Context are improved improvedmmm improved immmmmmmmmm im im im im im im shown are are Context Context Context Context Context Context', 'small right mid-lung\\n opacity, concerning for pneumonia.', 'small right mid-lung\\n opacity,', 'Not in context.']\n",
      "decoded_labels:---------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia', 'Right mid lung opacity', 'small right mid-lung\\n opacity, concerning for pneumonia', 'small right mid-lung\\n opacity', 'Not in context.']\n",
      "decoded_preds:----------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia.\\nimpl impl impl impl impl impl impl impl impl impl imag imag impl imag imag impl B in imag imagly', 'Right mid lung opacity is Context Context Context Context Context Context are are Not are Context Context are improved improvedmmm im immmmmmmmmm im im im im im im shown are are Context Context Context Context Context', 'small right mid-lung\\n opacity, concerning for pneumonia.', 'small right mid-lung\\n opacity,', 'Not in context.']\n",
      "decoded_labels:---------------------\n",
      " ['Right mid lung opacity is concerning for early pneumonia', 'Right mid lung opacity', 'small right mid-lung\\n opacity, concerning for pneumonia', 'small right mid-lung\\n opacity', 'Not in context.']\n"
     ]
    }
   ],
   "source": [
    "finetune(model, \n",
    "         radqa_train_dataset, #ct_impressions_train_dataset, \n",
    "         radqa_val_dataset,\n",
    "         peft_config, \n",
    "         1024, \n",
    "         tokenizer, \n",
    "         training_arguments, \n",
    "         packing, \n",
    "         generate_radqa_prompt, \n",
    "         new_model,\n",
    "         compute_metrics,\n",
    "         preprocess_logits_for_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f09fd9c88d146e2907db63e66f371e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34b16080bf71419fa6be4c53019ec1b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d3c1fd6fd44a728f2325d0a80dab2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ed5bee9b9a94da98d39d08121ab3a26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/437 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/imx2/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b090bd18805340c3aab370b55cc35443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d074542861c94746b81344cfefaf6481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc9a2ee921a4eeb9b862cab39099a51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/imxx/llama-2-7b-radnlp-chest-pelvis-mri-petct-radqa/commit/1b1523f10e4b4a86df82591b7607a3cac9d53c09', commit_message='Upload tokenizer', commit_description='', oid='1b1523f10e4b4a86df82591b7607a3cac9d53c09', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!huggingface-cli login --token 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\n",
    "\n",
    "model.push_to_hub(new_model, use_temp_dir=False)\n",
    "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [20:09<00:00,  2.42s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' Patent celiac and SMA, there is no ultrasound evidence of\\n stenosis. [**Female First Name (un',\n",
       " ' Patent celiac and SMA, there is no ultrasound evidence of\\n stenosis. [**Female First Name (un',\n",
       " ' Patent celiac and SMA, there is no ultrasound evidence of\\n stenosis. [**Female First Name (un',\n",
       " ' liver is normal in size and echogenicity without focal lesions.\\n There is no biliary duct dilatation.',\n",
       " '10.5 cm in length, the\\n left kidney measures 10.1 cm.  There is no splenomegaly',\n",
       " ' liver is normal in size and echogenicity without focal lesions.\\n There is no biliary duct dilatation.',\n",
       " '1) ET tube tip in good position, 2 cm above the carina.\\n\\n 2) Nasogatric tube',\n",
       " '1) ET tube tip in good position, 2 cm above the carina.\\n\\n 2) Nasogatric tube',\n",
       " '2 cm above the carina.  There is no\\n change in the position of the left IJ central venous catheter; the nas',\n",
       " '2 cm above the carina.  There is no\\n change in the position of the left IJ central venous catheter; the nas']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = get_radqas(contexts[:500], questions[:500], model, tokenizer, 30)\n",
    "predictions[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge1: 37.60160939194189\n",
      "Rouge2: 31.79404675210386\n",
      "RougeL: 36.74360415276978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert score: 24.885057040434912\n",
      "Average response lengths: {'prediction': 108.49, 'reference': 55.984}\n"
     ]
    }
   ],
   "source": [
    "rouge_scores = get_rouge_scores(predictions, answers[:500])\n",
    "print(f\"Rouge1: {rouge_scores['rouge1']['fmeasure_mean']}\")\n",
    "print(f\"Rouge2: {rouge_scores['rouge2']['fmeasure_mean']}\")\n",
    "print(f\"RougeL: {rouge_scores['rougeL']['fmeasure_mean']}\")\n",
    "bert_scores = get_bertscore(predictions, answers[:500], 7)\n",
    "print(f\"Bert score: {bert_scores['f1_mean']}\")\n",
    "avg_response_lengths = compare_lengths(predictions, answers[:500])\n",
    "print(f\"Average response lengths: {avg_response_lengths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
